{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "num_clients = 62\n",
    "num_iterations = 10\n",
    "alpha = 0.1\n",
    "lambda_param = 0.5\n",
    "sigma = 1.0\n",
    "batch_size = 32\n",
    "test_size_per_client = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def attention_derivative(wi, wj, sigma):\n",
    "    diff = np.linalg.norm(wi - wj)**2\n",
    "    return np.exp(-diff / sigma)\n",
    "\n",
    "\n",
    "def aggregate_models_amp(clients, alpha, sigma):\n",
    "    new_models = []\n",
    "    for i in range(num_clients):\n",
    "        wi = np.concatenate([p.data.cpu().numpy().ravel() for p in clients[i].parameters()])\n",
    "        weighted_sum = np.zeros_like(wi)\n",
    "        total_weight = 0\n",
    "        for j in range(num_clients):\n",
    "            if i != j:\n",
    "                wj = np.concatenate([p.data.cpu().numpy().ravel() for p in clients[j].parameters()])\n",
    "                weight = attention_derivative(wi, wj, sigma)\n",
    "                weighted_sum += weight * wj\n",
    "                total_weight += weight\n",
    "        new_model = (1 - alpha * total_weight) * wi + alpha * weighted_sum\n",
    "        new_models.append(new_model)\n",
    "        \n",
    "    new_model_states = []\n",
    "    for model_params in new_models:\n",
    "        model_state = {}  # assuming you're using a dictionary for model state dict\n",
    "        start = 0\n",
    "        for name, param in clients[0].named_parameters():\n",
    "            num_params = param.numel()\n",
    "            model_state[name] = torch.tensor(model_params[start:start + num_params]).reshape(param.shape)\n",
    "            start += num_params\n",
    "        new_model_states.append(model_state)\n",
    "    return new_models\n",
    "\n",
    "def aggregate_models_avg(clients):\n",
    "    avg_model = clients[0].state_dict()\n",
    "    for key in avg_model.keys():\n",
    "        avg_model[key] = torch.stack([clients[i].state_dict()[key] for i in range(num_clients)], dim=0).mean(dim=0)\n",
    "    return avg_model\n",
    "\n",
    "def local_update(model, cloud_model, optimizer, criterion, dataloader, alpha=None, lambda_param=None):\n",
    "    model.train()\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        if cloud_model and lambda_param:\n",
    "            model_params = np.concatenate([p.data.cpu().numpy().ravel() for p in model.parameters()])\n",
    "            cloud_params = np.concatenate([p.data.cpu().numpy().ravel() for p in cloud_model.parameters()])\n",
    "            regularization = lambda_param * np.sum((model_params - cloud_params)**2)\n",
    "            total_loss = loss + regularization\n",
    "        else:\n",
    "            total_loss = loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 62)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(torch.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"Start downloading dataset\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.EMNIST(root='./data', split='byclass', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='./data', split='byclass', train=False, download=True, transform=transform)\n",
    "\n",
    "def create_client_datasets(train_dataset, num_clients, test_size_per_client):\n",
    "    clients_data = [[] for _ in range(num_clients)]\n",
    "    digit_clients = list(range(10))\n",
    "    upper_clients = list(range(10, 36))\n",
    "    lower_clients = list(range(36, 62))\n",
    "    \n",
    "    for idx, (data, target) in enumerate(train_dataset):\n",
    "        if target < 10:\n",
    "            clients_data[np.random.choice(digit_clients)].append(idx)\n",
    "        elif 10 <= target < 36:\n",
    "            clients_data[np.random.choice(upper_clients)].append(idx)\n",
    "        elif 36 <= target < 62:\n",
    "            clients_data[np.random.choice(lower_clients)].append(idx)\n",
    "\n",
    "    client_train_datasets = []\n",
    "    client_test_datasets = []\n",
    "\n",
    "    for indices in clients_data:\n",
    "        train_indices, test_indices = random_split(indices, [len(indices) - test_size_per_client, test_size_per_client])\n",
    "        client_train_datasets.append(Subset(train_dataset, train_indices))\n",
    "        client_test_datasets.append(Subset(train_dataset, test_indices))\n",
    "\n",
    "    return client_train_datasets, client_test_datasets\n",
    "\n",
    "def assign_data_with_dominating_and_other_classes(train_dataset, clients_data, dominant_ratio=0.8):\n",
    "    client_train_datasets = []\n",
    "    client_test_datasets = []\n",
    "\n",
    "    for indices in clients_data:\n",
    "        dominant_size = int(len(indices) * dominant_ratio)\n",
    "        other_size = len(indices) - dominant_size\n",
    "\n",
    "        dominant_indices = indices[:dominant_size]\n",
    "        other_indices = indices[dominant_size:]\n",
    "\n",
    "        other_group_indices = []\n",
    "        for i, idx in enumerate(other_indices):\n",
    "            other_target = (train_dataset[idx][1] + np.random.randint(1, 62)) % 62  \n",
    "            other_group_indices.append(other_target)\n",
    "\n",
    "        train_indices, test_indices = random_split(indices, [len(indices) - test_size_per_client, test_size_per_client])\n",
    "        client_train_datasets.append(Subset(train_dataset, train_indices))\n",
    "        client_test_datasets.append(Subset(train_dataset, test_indices))\n",
    "\n",
    "    return client_train_datasets, client_test_datasets\n",
    "\n",
    "\n",
    "print(\"Create dataloaders\")\n",
    "\n",
    "client_train_datasets, client_test_datasets = create_client_datasets(train_dataset, num_clients, test_size_per_client)\n",
    "\n",
    "client_dataloaders = [DataLoader(client_train_datasets[i], batch_size=batch_size, shuffle=True) for i in range(num_clients)]\n",
    "client_test_dataloaders = [DataLoader(client_test_datasets[i], batch_size=batch_size, shuffle=False) for i in range(num_clients)]\n",
    "\n",
    "print(\"Client models initialized\")\n",
    "\n",
    "clients_models_amp = [SimpleCNN().to(device) for _ in range(num_clients)]\n",
    "clients_models_avg = [SimpleCNN().to(device) for _ in range(num_clients)]\n",
    "clients_optimizers_amp = [optim.SGD(model.parameters(), lr=0.01) for model in clients_models_amp]\n",
    "clients_optimizers_avg = [optim.SGD(model.parameters(), lr=0.01) for model in clients_models_avg]\n",
    "clients_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Cloud models initialized\")\n",
    "\n",
    "cloud_models_amp = [SimpleCNN().to(device) for _ in range(num_clients)]\n",
    "cloud_model_avg = SimpleCNN().to(device)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    accuracy = 100. * correct / len(dataloader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "all_accuracies_amp = []\n",
    "all_accuracies_avg = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "for i in range(num_clients):\n",
    "    print(i, end=' ')\n",
    "    local_update(clients_models_amp[i], cloud_models_amp[i], clients_optimizers_amp[i], clients_criterion, client_dataloaders[i], alpha, lambda_param)\n",
    "print(\"\")\n",
    "\n",
    "for i in range(num_clients):\n",
    "    print(i, end=' ')\n",
    "    local_update(clients_models_avg[i], None, clients_optimizers_avg[i], clients_criterion, client_dataloaders[i])\n",
    "print(\"\")\n",
    "\n",
    "accuracies_amp = []\n",
    "accuracies_avg = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    print(i, end=' ')\n",
    "    _, accuracy_amp = evaluate(clients_models_amp[i], client_test_dataloaders[i], clients_criterion)\n",
    "    _, accuracy_avg = evaluate(clients_models_avg[i], client_test_dataloaders[i], clients_criterion)\n",
    "    accuracies_amp.append(accuracy_amp)\n",
    "    accuracies_avg.append(accuracy_avg)\n",
    "print(\"\")\n",
    "\n",
    "avg_accuracy_amp = sum(accuracies_amp) / num_clients\n",
    "avg_accuracy_avg = sum(accuracies_avg) / num_clients\n",
    "all_accuracies_amp.append(avg_accuracy_amp)\n",
    "all_accuracies_avg.append(avg_accuracy_avg)\n",
    "\n",
    "print(f\"Iteration 0 : After Local Update - Avg Test Accuracy (FedAMP) = {avg_accuracy_amp:.2f}%, (FedAvg) = {avg_accuracy_avg:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_model_state = aggregate_models_amp(clients_models_amp, alpha, sigma)\n",
    "for i in range(num_clients):\n",
    "    print(i, end=' ')\n",
    "    cloud_models_amp[i].load_state_dict(amp_model_state[i])\n",
    "print(\"\")\n",
    "avg_model_state = aggregate_models_avg(clients_models_avg)\n",
    "for i in range(num_clients):\n",
    "    print(i, end=' ')\n",
    "    clients_models_avg[i].load_state_dict(avg_model_state)\n",
    "print(\"\")\n",
    "accuracies_amp = []\n",
    "accuracies_avg = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    print(i, end=' ')\n",
    "    _, accuracy_amp = evaluate(clients_models_amp[i], client_test_dataloaders[i], clients_criterion)\n",
    "    _, accuracy_avg = evaluate(clients_models_avg[i], client_test_dataloaders[i], clients_criterion)\n",
    "    accuracies_amp.append(accuracy_amp)\n",
    "    accuracies_avg.append(accuracy_avg)\n",
    "print(\"\")\n",
    "avg_accuracy_amp = sum(accuracies_amp) / num_clients\n",
    "avg_accuracy_avg = sum(accuracies_avg) / num_clients\n",
    "all_accuracies_amp.append(avg_accuracy_amp)\n",
    "all_accuracies_avg.append(avg_accuracy_avg)\n",
    "\n",
    "print(f\"Iteration {iteration}: After Cloud Update - Avg Test Accuracy (FedAMP) = {avg_accuracy_amp:.2f}%, (FedAvg) = {avg_accuracy_avg:.2f}%\")\n",
    "\n",
    "print(\"Avg accuracy over all iterations (FedAMP vs FedAvg):\")\n",
    "for idx in range(num_iterations):\n",
    "    print(f\"Iteration {idx}, After Local Update: FedAMP = {all_accuracies_amp[2*idx]:.2f}%, FedAvg = {all_accuracies_avg[2*idx]:.2f}%\")\n",
    "    print(f\"Iteration {idx}, After Cloud Update: FedAMP = {all_accuracies_amp[2*idx+1]:.2f}%, FedAvg = {all_accuracies_avg[2*idx+1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "results_dir = './results'\n",
    "fedamp_results_path = f'{results_dir}/fedamp_accuracies.pkl'\n",
    "fedavg_results_path = f'{results_dir}/fedavg_accuracies.pkl'\n",
    "\n",
    "with open(fedamp_results_path, 'wb') as f:\n",
    "    pickle.dump(all_accuracies_amp, f)\n",
    "\n",
    "with open(fedavg_results_path, 'wb') as f:\n",
    "    pickle.dump(all_accuracies_avg, f)\n",
    "\n",
    "print(f\"FedAMP accuracies saved to {fedamp_results_path}.\")\n",
    "print(f\"FedAvg accuracies saved to {fedavg_results_path}.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('./results/fedamp_accuracies.pkl', 'rb') as f:\n",
    "    fedamp_accuracies = pickle.load(f)\n",
    "\n",
    "with open('./results/fedavg_accuracies.pkl', 'rb') as f:\n",
    "    fedavg_accuracies = pickle.load(f)\n",
    "\n",
    "iterations = list(range(len(fedamp_accuracies)))\n",
    "\n",
    "plt.plot(iterations, fedamp_accuracies, 'blue', label='FedAMP')\n",
    "plt.plot(iterations, fedavg_accuracies, 'red', label='FedAvg')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('FedAMP vs FedAvg Accuracy')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.savefig('./results/fedamp_vs_fedavg_accuracy.png', dpi=200)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
